{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path):\n",
    "    \"\"\"Function used to create a Pandas DataFrame containing specifications page titles\n",
    "\n",
    "    Reads products specifications from the file system (\"dataset_path\" variable in the main function) and creates a Pandas DataFrame where each row is a\n",
    "    specification. The columns are 'source' (e.g. www.sourceA.com), 'spec_number' (e.g. 1) and the 'page title'. Note that this script will consider only\n",
    "    the page title attribute for simplicity.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): The path to the dataset\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The Pandas DataFrame containing specifications and page titles\n",
    "    \"\"\"\n",
    "\n",
    "    print('>>> Creating dataframe...\\n')\n",
    "    columns_df = ['source', 'spec_number', 'spec_id', 'page_title']\n",
    "\n",
    "    progressive_id = 0\n",
    "    progressive_id2row_df = {}\n",
    "    for source in tqdm(os.listdir(dataset_path)):\n",
    "        for specification in os.listdir(os.path.join(dataset_path, source)):\n",
    "            specification_number = specification.replace('.json', '')\n",
    "            specification_id = '{}//{}'.format(source, specification_number)\n",
    "            with open(os.path.join(dataset_path, source, specification)) as specification_file:\n",
    "                specification_data = json.load(specification_file)\n",
    "                page_title = specification_data.get('<page title>').lower()\n",
    "                row = (source, specification_number, specification_id, page_title)\n",
    "                progressive_id2row_df.update({progressive_id: row})\n",
    "                progressive_id += 1\n",
    "    df = pd.DataFrame.from_dict(progressive_id2row_df, orient='index', columns=columns_df)\n",
    "    print('>>> Dataframe created successfully!\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe('../datasets/unlabeled/2013_camera_specs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"source\", \"spec_number\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(['itself', 'down', 'by', 'with', 'doesn', 'wouldn', 'other', 'ours', 'of', 'then', 'where', 'don', 'these', 'nor', 'she', \"should've\", 'won', 'ma', 'from', 'had', \"you're\", 'our', 'did', 'them', 'too', 'her', 'that', 'haven', 'after', \"you'll\", 'hers', 'because', 'yourself', 'against', 'mightn', 'as', 'll', 'whom', 'how', 'couldn', 'further', 'aren', \"you'd\", 'and', 'needn', \"couldn't\", 'those', 'to', \"doesn't\", \"weren't\", 'both', 'ourselves', 'in', 'which', 'yours', 'under', 'some', 'what', 'during', 'before', \"needn't\", \"shan't\", 'here', 'having', 'hasn', 'your', \"hasn't\", 'between', 'me', \"she's\", 'into', 'all', 'at', 'shan', 'who', 'o', 'an', 'very', 'can', 'you', 'shouldn', 'such', 'but', 'do', 'out', 'am', \"shouldn't\", 'above', 'wasn', 'or', 'were', 'own', 'didn', \"you've\", 'on', 'will', 'my', 'it', 'have', 'once', 'only', 'been', 'themselves', 'his', 'be', \"mightn't\", 'they', 'not', 'so', 'up', 'any', 'most', 'has', 'myself', 't', 'yourselves', 'isn', \"it's\", 'y', 'm', 'now', 'until', 're', 'there', 'their', 'mustn', \"mustn't\", 'again', 'being', 'hadn', 'doing', 'just', 'no', 'if', 've', \"wasn't\", \"won't\", 'we', 'below', 'does', 'more', 'this', 'should', \"isn't\", 'ain', \"don't\", 'i', \"haven't\", 'than', \"didn't\", 'are', 'about', 'off', 'him', 'for', 'few', \"wouldn't\", 'was', 'weren', 'why', 'he', \"that'll\", 'd', 'the', 'its', 'a', 'each', 'is', 'while', \"aren't\", 'when', 'theirs', 'same', 's', 'himself', 'herself', \"hadn't\", 'through', 'over'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~€£¥₹₽\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation(word):\n",
    "    return ''.join(c for c in word if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"page_title\"] = df[\"page_title\"].apply(lambda x : [i.lower() for i in list(map(lambda y: replace_punctuation(y), word_tokenize(x))) if i and i.lower() not in stopWords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(\\S*[A-Za-z]\\S*[0-9]\\S*|\\S*[0-9]\\S*[A-Za-z]\\S*)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the data replace lumix with panasonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = ['360fly', 'acer', 'achiever', 'acorn', 'actionpro', 'activeon', 'aee', 'agfa', 'agfaphoto', 'aiptek', 'akaso', 'alpine', 'alpine', 'amkov', 'andoer', 'annke', 'ansco', 'apeman', 'apex', 'apple', 'archos', 'argus', 'arlo', 'arri', 'axis', 'bell', 'benq', 'blackmagic', 'blackmagic', 'bosch', 'bower', 'brinno', 'brookstone', 'browning', 'cambo', 'campark', 'canon', 'carl', 'casio', 'celestron', 'chinon', 'cisco', 'cobra', 'coleman', 'concord', 'contax', 'contour', 'covert', 'craig', 'crayola', 'creative', 'creative', 'crosstour', 'crumpler', 'datavideo', 'delkin', 'dell', 'digitrex', 'discovery', 'disney', 'dji', 'd-link', 'domke', 'dörr', 'dragon', 'dxg', 'dxo', 'easypix', 'elecom', 'elmo', 'emerson', 'energizer', 'epson', 'fisher-price', 'flip', 'flir', 'foscam', 'fotoman', 'fotopro', 'fuji', 'fujifilm', 'fujinon', 'garmin', 'gateway', 'godox', 'goodmans', 'google', 'gopro', 'grundig', 'hahnel', 'hamilton', 'hasselblad', 'hello', 'herofiber', 'hitachi', 'holga', 'horseman', 'hoya', 'htc', 'huawei', 'ikelite', 'ilford', 'impossible', 'innovage', 'insignia', 'insta360', 'intel', 'intova', 'ion', 'iris', 'jazz', 'jenoptik', 'jjrc', 'jvc', 'kaiser', 'kenko', 'keyence', 'king', 'kitvision', 'kodak', 'konica', 'kyocera', 'leaf', 'lego', 'leica', 'lenovo', 'lexibook', 'linhof', 'liquid', 'little', 'logitech', 'lomography', 'lowepro', 'ltl', 'lytro', 'maginon', 'magnavox', 'mamiya', 'manfrotto', 'marshall', 'marumi', 'mattel', 'maxell', 'meade', 'medion', 'memorex', 'mercury', 'metz', 'microsoft', 'microtek', 'midland', 'minolta', 'minox', 'monster', 'motorola', 'moultrie', 'mustek', 'nabi', 'neewer', 'nest', 'netgear', 'night', 'nikkon', 'nikkor', 'nikon', 'nilox', 'nintendo', 'nippon', 'nokia', 'norcent', 'olympus', 'optech', 'ordro', 'oregon', 'packard', 'palm', 'panasonic', 'parrot', 'pelco', 'pentacon', 'pentax', 'phase', 'philips', 'philips', 'phoenix', 'pioneer', 'playskool', 'polaroid', 'polarpro', 'praktica', 'premier', 'promaster', 'proscan', 'pyle', 'radioshack', 'raymarine', 'raynox', 'rca', 'ricoh', 'ring', 'rode', 'rokinon', 'rollei', 'ryobi', 'sakar', 'samsung', 'sandisk', 'sanyo', 'schneider', 'schneider', 'schneider', 'scosche', 'seasea', 'sealife', 'sharp', 'sharper', 'sigma', 'sinar', 'sipix', 'sjcam', 'sony', 'soocoo', 'stealth', 'superheadz', 'svp', 'swann', 'tamrac', 'tamron', 'technika', 'tenba', 'think', 'thule', 'tokina', 'tomy', 'toshiba', 'transcend', 'traveler', 'trust', 'verbatim', 'vibe', 'victure', 'vistaquest', 'vivitar', 'voigtländer', 'vtech', 'vupoint', 'walimex', 'wyze', 'xiaomi', 'xit', 'xtreme', 'yashica', 'zeiss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"page_title\"] = df[\"page_title\"].apply(lambda line : list(set(filter(lambda word : bool(pattern.match(word)) or word in brands,line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"brand\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many products have more than one brand\n",
    "for index, row in df.iterrows():\n",
    "    for brand in row[\"page_title\"]:\n",
    "        if brand in brands:\n",
    "            df[\"brand\"].iloc[index] = brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row[\"brand\"] == \"\":\n",
    "        df[\"brand\"].iloc[index] = \"unbranded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO manage entries with multiple brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby([\"brand\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorith for every brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "for group_name, group in df:\n",
    "    print(\"CALCULATING FOR BRAND = \", group_name)\n",
    "    merged = group.drop(columns=[\"brand\"], axis = 1)\n",
    "    merged = (merged.merge(merged, on=merged.assign(key_col=1)['key_col'], suffixes=('', '_right'))\n",
    " .query('spec_id != spec_id_right') # filter out joins on the same row\n",
    " .reset_index(drop=True))\n",
    "    merged.drop(columns = [\"key_0\"], axis = 1, inplace=True)\n",
    "    merged[\"zipped\"] = tuple(map(lambda line : sorted(line), list(zip(merged[\"spec_id\"], merged[\"spec_id_right\"]))))\n",
    "    merged.drop_duplicates(\"zipped\", inplace=True)\n",
    "    merged.drop(columns=[\"zipped\"], inplace=True)\n",
    "    merged.rename(columns = {\"spec_id\" : \"left_spec_id\", \"spec_id_right\" : \"right_spec_id\"}, inplace=True)\n",
    "    merged.reset_index(inplace=True)\n",
    "    merged[\"label\"] = 0\n",
    "    print(\"NUMBER OF COMPARISONS: \", len(merged))\n",
    "    commons = 0\n",
    "    for index, row in merged.iterrows():\n",
    "        if index % 100000 == 0 and index != 0:\n",
    "            print(index)\n",
    "        for spec in row[\"page_title\"]:\n",
    "            if spec in row[\"page_title_right\"]:\n",
    "                commons += 1\n",
    "        if commons >= 2:\n",
    "            merged[\"label\"].iloc[index] = 1\n",
    "    \n",
    "        commons = 0\n",
    "    merged = merged.loc[merged['label'] == 1]\n",
    "    merged.drop(columns=[\"page_title\", \"page_title_right\", \"label\"], axis = 1).to_csv(\"../datasets/created/brand_{}_compact_matches_labeled.csv\".format(group_name),index = False)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print(\"ALL TOGETHER IT TOOK : \", elapsed, \"SECONDS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
