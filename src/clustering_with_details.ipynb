{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import ast\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dataset_path):\n",
    "    \"\"\"Function used to create a Pandas DataFrame containing specifications page titles\n",
    "\n",
    "    Reads products specifications from the file system (\"dataset_path\" variable in the main function) and creates a Pandas DataFrame where each row is a\n",
    "    specification. The columns are 'source' (e.g. www.sourceA.com), 'spec_number' (e.g. 1) and the 'page title'. Note that this script will consider only\n",
    "    the page title attribute for simplicity.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): The path to the dataset\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The Pandas DataFrame containing specifications and page titles\n",
    "    \"\"\"\n",
    "\n",
    "    print('>>> Creating dataframe...\\n')\n",
    "    columns_df = ['source', 'spec_number', 'spec_id', 'page_title']\n",
    "\n",
    "    progressive_id = 0\n",
    "    progressive_id2row_df = {}\n",
    "    for source in tqdm(os.listdir(dataset_path)):\n",
    "        for specification in os.listdir(os.path.join(dataset_path, source)):\n",
    "            specification_number = specification.replace('.json', '')\n",
    "            specification_id = '{}//{}'.format(source, specification_number)\n",
    "            with open(os.path.join(dataset_path, source, specification)) as specification_file:\n",
    "                specification_data = json.load(specification_file)\n",
    "                page_title = specification_data.get('<page title>').lower()\n",
    "                row = (source, specification_number, specification_id, page_title)\n",
    "                progressive_id2row_df.update({progressive_id: row})\n",
    "                progressive_id += 1\n",
    "    df = pd.DataFrame.from_dict(progressive_id2row_df, orient='index', columns=columns_df)\n",
    "    print('>>> Dataframe created successfully!\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe('../datasets/unlabeled/2013_camera_specs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"source\", \"spec_number\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(['itself', 'down', 'by', 'with', 'doesn', 'wouldn', 'other', 'ours', 'of', 'then', 'where', 'don', 'these', 'nor', 'she', \"should've\", 'won', 'ma', 'from', 'had', \"you're\", 'our', 'did', 'them', 'too', 'her', 'that', 'haven', 'after', \"you'll\", 'hers', 'because', 'yourself', 'against', 'mightn', 'as', 'll', 'whom', 'how', 'couldn', 'further', 'aren', \"you'd\", 'and', 'needn', \"couldn't\", 'those', 'to', \"doesn't\", \"weren't\", 'both', 'ourselves', 'in', 'which', 'yours', 'under', 'some', 'what', 'during', 'before', \"needn't\", \"shan't\", 'here', 'having', 'hasn', 'your', \"hasn't\", 'between', 'me', \"she's\", 'into', 'all', 'at', 'shan', 'who', 'o', 'an', 'very', 'can', 'you', 'shouldn', 'such', 'but', 'do', 'out', 'am', \"shouldn't\", 'above', 'wasn', 'or', 'were', 'own', 'didn', \"you've\", 'on', 'will', 'my', 'it', 'have', 'once', 'only', 'been', 'themselves', 'his', 'be', \"mightn't\", 'they', 'not', 'so', 'up', 'any', 'most', 'has', 'myself', 't', 'yourselves', 'isn', \"it's\", 'y', 'm', 'now', 'until', 're', 'there', 'their', 'mustn', \"mustn't\", 'again', 'being', 'hadn', 'doing', 'just', 'no', 'if', 've', \"wasn't\", \"won't\", 'we', 'below', 'does', 'more', 'this', 'should', \"isn't\", 'ain', \"don't\", 'i', \"haven't\", 'than', \"didn't\", 'are', 'about', 'off', 'him', 'for', 'few', \"wouldn't\", 'was', 'weren', 'why', 'he', \"that'll\", 'd', 'the', 'its', 'a', 'each', 'is', 'while', \"aren't\", 'when', 'theirs', 'same', 's', 'himself', 'herself', \"hadn't\", 'through', 'over'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~€£¥₹₽\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation(word):\n",
    "    return ''.join(c for c in word if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"page_title\"] = df[\"page_title\"].apply(lambda x : [i.lower() for i in list(map(lambda y: replace_punctuation(y), word_tokenize(x))) if i and i.lower() not in stopWords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(\\S*[A-Za-z]\\S*[0-9]\\S*|\\S*[0-9]\\S*[A-Za-z]\\S*)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the data replace lumix with panasonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = ['360fly', 'acer', 'achiever', 'acorn', 'actionpro', 'activeon', 'aee', 'agfa', 'agfaphoto', 'aiptek', 'akaso', 'alpine', 'alpine', 'amkov', 'andoer', 'annke', 'ansco', 'apeman', 'apex', 'apple', 'archos', 'argus', 'arlo', 'arri', 'axis', 'bell', 'benq', 'blackmagic', 'blackmagic', 'bosch', 'bower', 'brinno', 'brookstone', 'browning', 'cambo', 'campark', 'canon', 'carl', 'casio', 'celestron', 'chinon', 'cisco', 'cobra', 'coleman', 'concord', 'contax', 'contour', 'covert', 'craig', 'crayola', 'creative', 'creative', 'crosstour', 'crumpler', 'datavideo', 'delkin', 'dell', 'digitrex', 'discovery', 'disney', 'dji', 'd-link', 'domke', 'dörr', 'dragon', 'dxg', 'dxo', 'easypix', 'elecom', 'elmo', 'emerson', 'energizer', 'epson', 'fisher-price', 'flip', 'flir', 'foscam', 'fotoman', 'fotopro', 'fuji', 'fujifilm', 'fujinon', 'garmin', 'gateway', 'godox', 'goodmans', 'google', 'gopro', 'grundig', 'hahnel', 'hamilton', 'hasselblad', 'hello', 'herofiber', 'hitachi', 'holga', 'horseman', 'hoya', 'htc', 'huawei', 'ikelite', 'ilford', 'impossible', 'innovage', 'insignia', 'insta360', 'intel', 'intova', 'ion', 'iris', 'jazz', 'jenoptik', 'jjrc', 'jvc', 'kaiser', 'kenko', 'keyence', 'king', 'kitvision', 'kodak', 'konica', 'kyocera', 'leaf', 'lego', 'leica', 'lenovo', 'lexibook', 'linhof', 'liquid', 'little', 'logitech', 'lomography', 'lowepro', 'ltl', 'lytro', 'maginon', 'magnavox', 'mamiya', 'manfrotto', 'marshall', 'marumi', 'mattel', 'maxell', 'meade', 'medion', 'memorex', 'mercury', 'metz', 'microsoft', 'microtek', 'midland', 'minolta', 'minox', 'monster', 'motorola', 'moultrie', 'mustek', 'nabi', 'neewer', 'nest', 'netgear', 'night', 'nikkon', 'nikkor', 'nikon', 'nilox', 'nintendo', 'nippon', 'nokia', 'norcent', 'olympus', 'optech', 'ordro', 'oregon', 'packard', 'palm', 'panasonic', 'parrot', 'pelco', 'pentacon', 'pentax', 'phase', 'philips', 'philips', 'phoenix', 'pioneer', 'playskool', 'polaroid', 'polarpro', 'praktica', 'premier', 'promaster', 'proscan', 'pyle', 'radioshack', 'raymarine', 'raynox', 'rca', 'ricoh', 'ring', 'rode', 'rokinon', 'rollei', 'ryobi', 'sakar', 'samsung', 'sandisk', 'sanyo', 'schneider', 'schneider', 'schneider', 'scosche', 'seasea', 'sealife', 'sharp', 'sharper', 'sigma', 'sinar', 'sipix', 'sjcam', 'sony', 'soocoo', 'stealth', 'superheadz', 'svp', 'swann', 'tamrac', 'tamron', 'technika', 'tenba', 'think', 'thule', 'tokina', 'tomy', 'toshiba', 'transcend', 'traveler', 'trust', 'verbatim', 'vibe', 'victure', 'vistaquest', 'vivitar', 'voigtländer', 'vtech', 'vupoint', 'walimex', 'wyze', 'xiaomi', 'xit', 'xtreme', 'yashica', 'zeiss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"page_title\"] = df[\"page_title\"].apply(lambda line : list(set(filter(lambda word : bool(pattern.match(word)) or word in brands,line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"brand\"] = [[] for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create brand column\n",
    "for index, row in df.iterrows():\n",
    "    for brand in row[\"page_title\"]:\n",
    "        if brand in brands:\n",
    "            if not brand in df.at[index, \"brand\"]:\n",
    "                df.at[index, \"brand\"].append(brand)\n",
    "                row[\"page_title\"].remove(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mp_mm_g_oz(value):\n",
    "    if not isinstance(value, list) and pd.isna(value):\n",
    "        return np.nan\n",
    "    regex = r\"[0-9]+mm(\\n|)\"\n",
    "    regex2 = r\"[0-9]+mp(\\n|)\"\n",
    "    regex3 = r\"[0-9]+oz\"\n",
    "    regex4 = r\"[0-9]+g(\\n|)$\"\n",
    "    repl = value\n",
    "    for e in repl:\n",
    "        if bool(re.match(regex, e)) or bool(re.match(regex2, e)) or bool(re.match(regex3, e)) or bool(re.match(regex4, e)):\n",
    "            repl.remove(e)\n",
    "    return repl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"page_title\"] = df[\"page_title\"].apply(lambda row : clean_mp_mm_g_oz(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "os.chdir(\"../datasets/unlabeled/cleaned\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "df_cleaned = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "#export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge clean with title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_cleaned, on=\"spec_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"brand_x\" : \"brand_from_title\", \"brand_y\" : \"brand_descr\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_short_descr(line):\n",
    "    pattern = re.compile(\"(\\S*[A-Za-z]\\S*[0-9]\\S*|\\S*[0-9]\\S*[A-Za-z]\\S*)\")\n",
    "    brands = ['360fly', 'acer', 'achiever', 'acorn', 'actionpro', 'activeon', 'aee', 'agfa', 'agfaphoto', 'aiptek', 'akaso', 'alpine', 'alpine', 'amkov', 'andoer', 'annke', 'ansco', 'apeman', 'apex', 'apple', 'archos', 'argus', 'arlo', 'arri', 'axis', 'bell', 'benq', 'blackmagic', 'blackmagic', 'bosch', 'bower', 'brinno', 'brookstone', 'browning', 'cambo', 'campark', 'canon', 'carl', 'casio', 'celestron', 'chinon', 'cisco', 'cobra', 'coleman', 'concord', 'contax', 'contour', 'covert', 'craig', 'crayola', 'creative', 'creative', 'crosstour', 'crumpler', 'datavideo', 'delkin', 'dell', 'digitrex', 'discovery', 'disney', 'dji', 'd-link', 'domke', 'dörr', 'dorr', 'dragon', 'dxg', 'dxo', 'easypix', 'elecom', 'elmo', 'emerson', 'energizer', 'epson', 'fisher-price', 'flip', 'flir', 'foscam', 'fotoman', 'fotopro', 'fuji', 'fujifilm', 'fujinon', 'garmin', 'gateway', 'godox', 'goodmans', 'google', 'gopro', 'grundig', 'hahnel', 'hamilton', 'hasselblad', 'hello', 'herofiber', 'hitachi', 'holga', 'horseman', 'hoya', 'htc', 'huawei', 'ikelite', 'ilford', 'impossible', 'innovage', 'insignia', 'insta360', 'intel', 'intova', 'ion', 'iris', 'jazz', 'jenoptik', 'jjrc', 'jvc', 'kaiser', 'kenko', 'keyence', 'king', 'kitvision', 'kodak', 'konica', 'kyocera', 'leaf', 'lego', 'leica', 'lenovo', 'lexibook', 'linhof', 'liquid', 'logitech', 'lomography', 'lowepro', 'ltl', 'lytro', 'maginon', 'magnavox', 'mamiya', 'manfrotto', 'marshall', 'marumi', 'mattel', 'maxell', 'meade', 'medion', 'memorex', 'mercury', 'metz', 'microsoft', 'microtek', 'midland', 'minolta', 'minox', 'monster', 'motorola', 'moultrie', 'mustek', 'nabi', 'neewer', 'nest', 'netgear', 'nikkon', 'nikkor', 'nikon', 'nilox', 'nintendo', 'nippon', 'nokia', 'norcent', 'olympus', 'optech', 'ordro', 'oregon', 'packard', 'palm', 'panasonic', 'parrot', 'pelco', 'pentacon', 'pentax', 'phase', 'philips', 'philips', 'phoenix', 'pioneer', 'playskool', 'polaroid', 'polarpro', 'praktica', 'premier', 'promaster', 'proscan', 'pyle', 'radioshack', 'raymarine', 'raynox', 'rca', 'ricoh', 'ring', 'rode', 'rokinon', 'rollei', 'ryobi', 'sakar', 'samsung', 'sandisk', 'sanyo', 'schneider', 'schneider', 'schneider', 'scosche', 'seasea', 'sealife', 'sharp', 'sharper', 'sigma', 'sinar', 'sipix', 'sjcam', 'sony', 'soocoo', 'stealth', 'superheadz', 'svp', 'swann', 'tamrac', 'tamron', 'technika', 'tenba', 'think', 'thule', 'tokina', 'tomy', 'toshiba', 'transcend', 'traveler', 'trust', 'verbatim', 'vibe', 'victure', 'vistaquest', 'vivitar', 'voigtländer', 'vtech', 'vupoint', 'walimex', 'wyze', 'xiaomi', 'xit', 'xtreme', 'yashica', 'zeiss']\n",
    "    if not isinstance(line, list) and pd.isna(line):\n",
    "        return np.nan\n",
    "    else:\n",
    "        line = ast.literal_eval(line)\n",
    "        return list(set(filter(lambda word : bool(pattern.match(word)) or word in brands,line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"short_descr\"] = df[\"short_descr\"].apply(clean_short_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"short_descr\"] = df[\"short_descr\"].apply(lambda row : clean_mp_mm_g_oz(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add units to megapixels and screen_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"megapixels\"] = df[\"megapixels\"].apply(lambda value: str(value) + \"mp\" if not pd.isna(value) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"screen_size\"] = df[\"screen_size\"].apply(lambda value: str(value) + \"in\" if not pd.isna(value) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weight\"] = df[\"weight\"].apply(lambda value: str(value) + \"g\" if not pd.isna(value) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title_brands(title):\n",
    "    new_title = []\n",
    "    for e in title:\n",
    "        if 'fuji' in e:\n",
    "            new_title.append(\"fuji\")\n",
    "        elif \"nikkor\" in e or \"nikkon\" in e:\n",
    "            new_title.append(\"nikon\")\n",
    "        elif \"butterfly\" in e:\n",
    "            new_title.append(\"butterfly\")\n",
    "        elif \"blackmagic\" in e:\n",
    "            new_title.append(\"blackmagic\")\n",
    "        else:\n",
    "            new_title.append(e)\n",
    "    return list(set(new_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"brand_from_title\"] = df[\"brand_from_title\"].apply(clean_title_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_manufacturer(e):\n",
    "    if pd.isna(e):\n",
    "        return e\n",
    "    if 'fuji' in e:\n",
    "        return \"fuji\"\n",
    "    elif \"nikkor\" in e or \"nikkon\" in e or \"niko9\" in e:\n",
    "        return \"nikon\"\n",
    "    elif \"penx9\" in e:\n",
    "        return \"pentax\"\n",
    "    elif \"canu9\" in e or \"canon cameras us\" in e:\n",
    "        return \"canon\"\n",
    "    elif \"butterfly\" in e:\n",
    "        return \"butterfly\"\n",
    "    elif \"blackmagic\" in e:\n",
    "        return \"blackmagic\"\n",
    "    elif \"leica camera\" in e:\n",
    "        return \"leica\"\n",
    "    elif \"samsung pleomax zirex\" in e:\n",
    "        return \"samsung\"\n",
    "    elif \"digital\" in e or \"lomo cameras\" in e or \"micro solution of japan\" in e or \"ricoh cameras usa\" in e:\n",
    "        np.nan\n",
    "    else:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"manufacturer\"] = df[\"manufacturer\"].apply(clean_manufacturer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_type(camera):\n",
    "    if pd.isna(camera):\n",
    "        return camera\n",
    "    if \"slr\" in camera:\n",
    "        return \"dslr\"\n",
    "    elif \"point shoot\" in camera:\n",
    "        return \"point shoot\"\n",
    "    elif \"compact\" in camera:\n",
    "        return \"compact\"\n",
    "    elif \"mirrorless\" in camera:\n",
    "        return \"mirrorless\"\n",
    "    else:\n",
    "        return camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"type\"] = df[\"type\"].apply(clean_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"brand_descr\"] = df[\"brand_descr\"].apply(clean_manufacturer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge brands together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brands_column(row):\n",
    "    repl = row[\"brand_from_title\"]\n",
    "    if not pd.isna(row[\"brand_descr\"]):\n",
    "        repl.append(row[\"brand_descr\"])\n",
    "    if not pd.isna(row[\"manufacturer\"]):\n",
    "        repl.append(row[\"manufacturer\"])\n",
    "    return tuple(set(repl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"merged_brands\"] = df.apply(create_brands_column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.apply(lambda row : row[\"page_title\"] == [], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"brand_from_title\", \"brand_descr\", \"manufacturer\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled = pd.read_csv(\"/Users/gfotiadis/programming/sigmod/datasets/created/with_details/combined_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_joined = labeled.merge(df, left_on='left_spec_id', right_on='spec_id')\n",
    "# left_joined.rename(columns={'page_title': 'left_page_title'}, inplace=True)\n",
    "# left_joined.drop('spec_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right_joined = labeled.merge(df, left_on='right_spec_id', right_on='spec_id')\n",
    "# right_joined.rename(columns={'page_title': 'right_page_title'}, inplace=True)\n",
    "# right_joined.drop('spec_id', axis=1, inplace=True)\n",
    "# right_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully_joined = pd.merge(left_joined, right_joined, how=\"inner\", left_on=[\"left_spec_id\", \"right_spec_id\"], right_on=[\"left_spec_id\", \"right_spec_id\"], suffixes=(\"_left\", \"_right\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully_joined.right_page_title.apply(lambda title : np.nan if title == [] else title).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully_joined.left_page_title.apply(lambda title : np.nan if title == [] else title).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"merged_brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbranded_til_100 = grouped.get_group(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gname, group in grouped:\n",
    "    if len(group) < 100:\n",
    "        unbranded_til_100 = pd.concat([group, unbranded_til_100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unbranded_til_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_df(dataframe):\n",
    "    merged = dataframe.drop(columns=[\"merged_brands\"], axis = 1)\n",
    "    merged = (merged.merge(merged, on=merged.assign(key_col=1)['key_col'], suffixes=('', '_right'))\n",
    " .query('spec_id < spec_id_right') # filter out joins on the same row and keep unique combinations\n",
    " .reset_index(drop=True))\n",
    "    merged.drop(columns = [\"key_0\"], axis = 1, inplace=True)\n",
    "    merged.rename(columns = {\"spec_id\" : \"left_spec_id\", \"spec_id_right\" : \"right_spec_id\"}, inplace=True)\n",
    "    merged.reset_index(inplace=True)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_match(row):\n",
    "    if row[\"page_title\"] == [] or row[\"page_title_right\"] == []:\n",
    "        target = 0.3\n",
    "    else:\n",
    "        target = 0.9\n",
    "        \n",
    "    #print(row.isna().sum(), len(row))\n",
    "    \n",
    "    #not_nan_per = row.isna().sum() / len(row)\n",
    "\n",
    "    \n",
    "    dim_weight = 0.95\n",
    "    dots_weight = 0.98\n",
    "    mp_weight = 0.44\n",
    "    scr_weight = 0.54\n",
    "    type_weight = 0.49\n",
    "    weight_weight = 0.81\n",
    "    descr_weight= 0.94\n",
    "    title_weight = 1.2\n",
    "    \n",
    "    \n",
    "    score = 0\n",
    "    dim_l = row[\"dimensions\"]\n",
    "    dim_r = row[\"dimensions_right\"]\n",
    "    dots_l = row[\"dots\"]\n",
    "    dots_r = row[\"dots_right\"]\n",
    "    megapixels_l = row[\"megapixels\"]\n",
    "    megapixels_r = row[\"megapixels_right\"]\n",
    "    screen_size_l = row[\"screen_size\"]\n",
    "    screen_size_r = row[\"screen_size_right\"]\n",
    "    short_descr_l = row[\"short_descr\"]\n",
    "    short_descr_r = row[\"short_descr_right\"]\n",
    "    type_l = row[\"type\"]\n",
    "    type_r = row[\"type_right\"]\n",
    "    weight_l = row[\"weight\"]\n",
    "    weight_r = row[\"weight_right\"]\n",
    "    page_title_l = row[\"page_title\"]\n",
    "    page_title_r = row[\"page_title_right\"]\n",
    "    \n",
    "    dimensions_regex = r\"([0-9]+\\.[0-9]+|[0-9]+)h([0-9]+\\.[0-9]+|[0-9]+)w([0-9]+\\.[0-9]+|[0-9]+)d\"\n",
    "    dimensions_regex_2 = r\"h([0-9]+\\.[0-9]+|[0-9]+)w([0-9]+\\.[0-9]+|[0-9]+)d([0-9]+\\.[0-9]+|[0-9]+)\"\n",
    "    \n",
    "    if not pd.isna(dim_l) and not pd.isna(dim_r):\n",
    "        if re.match(dimensions_regex, dim_l) == None:\n",
    "            groups_l = re.match(dimensions_regex_2, dim_l).groups(1)\n",
    "        else:\n",
    "            groups_l = re.match(dimensions_regex, dim_l).groups(1)\n",
    "        if re.match(dimensions_regex, dim_r) == None:\n",
    "            groups_r = re.match(dimensions_regex_2, dim_r).groups(1)\n",
    "        else:\n",
    "            groups_r = re.match(dimensions_regex, dim_r).groups(1)\n",
    "        if np.sum(np.abs(np.array(groups_l).astype(float) - np.array(groups_r).astype(float))) <= 0.3:\n",
    "            score += 0.95\n",
    "    if not pd.isna(dots_l) and not pd.isna(dots_r) and dots_l == dots_r:\n",
    "        score += 0.98\n",
    "    if not pd.isna(megapixels_l) and not pd.isna(megapixels_r) and abs(float(megapixels_l.replace(\"mp\", \"\")) - float(megapixels_r.replace(\"mp\", \"\"))) <= 0.2:\n",
    "        score += 0.44\n",
    "    if not pd.isna(screen_size_l) and not pd.isna(screen_size_r) and abs(float(screen_size_l.replace(\"in\", \"\")) - float(screen_size_r.replace(\"in\", \"\"))) <= 0.2:\n",
    "        score += 0.54\n",
    "    if not pd.isna(type_l) and not pd.isna(type_r) and type_l == type_r:\n",
    "        score += 0.5\n",
    "    if not pd.isna(weight_l) and not pd.isna(weight_r) and abs(float(weight_l.replace(\"g\", \"\")) - float(weight_r.replace(\"g\", \"\"))) <= 0.2:\n",
    "        score += 0.81\n",
    "        \n",
    "    if isinstance(page_title_r, list) and isinstance(short_descr_l, list) and short_descr_l == short_descr_r:\n",
    "        for spec1 in short_descr_l:\n",
    "            for spec2 in short_descr_r:\n",
    "                if spec1 == spec2:  \n",
    "                    score += 0.94\n",
    "    if isinstance(page_title_r, list) and isinstance(page_title_l, list) and page_title_r == page_title_l:\n",
    "        for spec1 in page_title_l:\n",
    "            for spec2 in page_title_r:\n",
    "                if spec1 == spec2:  \n",
    "                    score += 1.2\n",
    "   \n",
    "    return score >= target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gname, group in grouped:\n",
    "    labels = []\n",
    "    \n",
    "    if len(group) == 1 or gname == ():\n",
    "        continue\n",
    "    #brand_and_unbranded = pd.concat([group, unbranded])\n",
    "    \n",
    "    print(\"CALCULATING FOR BRAND = \", gname)\n",
    "    merged = get_merged_df(group)\n",
    "\n",
    "    #logic\n",
    "    \n",
    "    print(\"NUMBER OF COMPARISONS: \", len(merged))\n",
    "    labels.append(list(merged.progress_apply(determine_match, axis = 1)))\n",
    "    labels = sum(labels, [])\n",
    "    merged[\"label\"] = labels\n",
    "    print(\"MATCHED \", sum(merged[\"label\"]), \" OUT OF \", len(merged[\"label\"]))\n",
    "    del labels\n",
    "    merged = merged.loc[merged['label'] == True]\n",
    "    cols = [\"left_spec_id\", \"right_spec_id\"]\n",
    "    merged = merged[cols]\n",
    "    if not '/' in gname[0]:\n",
    "        merged.to_csv(\"/Users/gfotiadis/programming/sigmod/datasets/created/with_details/{}_matches_labeled.csv\".format(gname), index = False)\n",
    "    else:\n",
    "        merged.to_csv(\"/Users/gfotiadis/programming/sigmod/datasets/created/with_details/{}_matches_labeled.csv\".format(gname[0].replace(\"/\", \"\")), index = False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unbranded = []\n",
    "merged_unbranded = get_merged_df(unbranded_til_100)\n",
    "labels_unbranded.append(list(merged_unbranded.progress_apply(determine_match, axis = 1)))\n",
    "labels_unbranded = sum(labels_unbranded, [])\n",
    "merged_unbranded[\"label\"] = labels_unbranded\n",
    "del labels_unbranded\n",
    "merged_unbranded = merged_unbranded.loc[merged_unbranded['label'] == True]\n",
    "cols = [\"left_spec_id\", \"right_spec_id\"]\n",
    "merged_unbranded = merged_unbranded[cols]\n",
    "merged_unbranded.to_csv(\"/Users/gfotiadis/programming/sigmod/datasets/created/with_details/unbranded_matches_labeled.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_unbranded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "os.chdir(\"/Users/gfotiadis/programming/sigmod/datasets/created/with_details/\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "combined_csv = pd.concat([pd.read_csv(f, header = 0) for f in all_filenames ])\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"/Users/gfotiadis/programming/sigmod/datasets/created/with_details/combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old 15100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 600,212"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
